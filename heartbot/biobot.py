# -*- coding: utf-8 -*-
"""biobot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Df-ExE0DlNWoQdEjva2MVoUeZrfdEwaL
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

pip install langchain-community langchain-core

"""#importing libraries"""

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA,LLMChain

!pip install langchain_community

"""##import pdf"""

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/biobot") #change path
docs = loader.load()

len(docs)

docs[4]

"""##chunks"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

len(chunks)

chunks[16]

"""##Embedding creation"""

import os
os.environ['HUGGINGFACEHUB_AIP_TOKEN'] = "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"

embeddings = SentenceTransformerEmbeddings(model_name="NeuML/pubmedbert-base-embeddings")

"""##vector store creation"""

VectorStore = Chroma.from_documents(chunks,embeddings)#hybrid search

query = "which age people will have heart disease"
search_results = VectorStore.similarity_search(query)

search_results

retriever = VectorStore.as_retriever(search_kwargs={'k':3}) #fetching results retriever

retriever.get_relevant_documents(query)

"""##LLM model loading"""

llm = LlamaCpp(
    model_path ="/content/drive/MyDrive/biobot/BioMistral-7B.Q4_K_M(1).gguf",
    temperature = 0.2,
    max_tokens = 2048,
    top_p = 1,
)

"""##use LLM ,retriver, query to give final output"""

template = """
<|context|>
you are an Medical Assistant that follows the instructions and generate the accurate response based on the query and the context provided.
please be truthful and give direct answers.be kind
</s>  #format separater
<|user|>  # user query will be shown here
{query}
</s>
<|assistant|>
"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke(query)

response

import sys

while True:
  user_input = input(f"any question?:")
  if user_input == "exit":
    sys.exit()
  if user_input=="":
    continue
  result = rag_chain.invoke(user_input)
  print("answer: ",result)

